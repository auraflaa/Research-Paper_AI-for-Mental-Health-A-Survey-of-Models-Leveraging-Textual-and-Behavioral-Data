# AI for Mental Health
## A Survey of Models Leveraging Textual and Behavioral Data

This repository contains the **complete reproduction package** for the systematic survey:

> **â€œAI for Mental Health: A Survey of Models Leveraging Textual and Behavioral Dataâ€**

The work examines AI-based detection of **Anxiety, Depression, and Stress (ADS)** using textual and behavioral signals, with a particular focus on **methodological rigor**, **evaluation validity**, and **deployment readiness** rather than headline accuracy improvements.

---

## ğŸ“Œ Abstract

Recent advances in Deep Learning for ADS detection frequently report substantial performance gains over classical approaches. However, through a systematic review and paired statistical analysis, we identify a **performance saturation plateau**â€”where increasingly complex architectures yield **diminishing returns** once controls for **data leakage**, **evaluation protocol rigor**, and **sample provenance** are enforced.

Using paired within-study comparisons, we observe statistically significant saturation:

- **Wilcoxon Signed-Rank Test:** p = 0.0244  
- **Cliffâ€™s Delta:** 0.5950 (large effect size)

To address the persistent gap between **research prototypes** and **deployable clinical systems**, we introduce an **Operational Readiness Checklist (ORC)**â€”a structured auditing framework for assessing whether a model meets minimum standards for responsible deployment.

---

## ğŸ“‚ Repository Structure

```text
Research-Paper_AI-for-Mental-Health/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ screening_log.csv        # PRISMA screening records (N = 92)
â”‚   â”œâ”€â”€ study_extraction.csv     # Structured feature extraction & metrics (N = 27)
â”‚   â”œâ”€â”€ paired_comparisons.csv   # Paired results for saturation hypothesis testing
â”‚   â””â”€â”€ orc_report.csv           # Generated Operational Readiness audit report
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ validate_prism.py        # Validates PRISMA data integrity and synchronization
â”‚   â”œâ”€â”€ analyze_saturation.py   # Statistical analysis (Wilcoxon, Cliffâ€™s Delta)
â”‚   â”œâ”€â”€ generate_orc.py          # Generates Operational Readiness (ORC) CSV reports
â”‚   â””â”€â”€ generate_prisma.py      # Automates PRISMA 2020 flowchart generation
â”‚
â”œâ”€â”€ figures/
â”‚   â”œâ”€â”€ saturation_plot.pdf      # Empirical evidence of performance saturation
â”‚   â”œâ”€â”€ prisma_flow.pdf          # PRISMA 2020 study inclusion flowchart
â”‚   â””â”€â”€ prisma_flow.tex          # LaTeX/TikZ source for the PRISMA diagram
â”‚
â”œâ”€â”€ refs/                        # Primary study PDFs and metadata placeholders
â”œâ”€â”€ requirements.txt             # Python dependencies (pandas, scipy, graphviz, etc.)
â”œâ”€â”€ LICENSE                      # MIT License
â””â”€â”€ README.md                    # Project overview and reproduction guide
```

---

## ğŸš€ Reproduction Workflow

### 1. Environment Setup

```bash
git clone https://github.com/auraflaa/Research-Paper_AI-for-Mental-Health-A-Survey-of-Models-Leveraging-Textual-and-Behavioral-Data.git
cd Research-Paper_AI-for-Mental-Health-A-Survey-of-Models-Leveraging-Textual-and-Behavioral-Data
python -m venv venv
.\venv\Scripts\activate
pip install -r requirements.txt
```

### 2. Data Integrity Check

Verify that the PRISMA screening log is synchronized with the extraction ledger.

```bash
python scripts/validate_prism.py
```

**Expected output:**

```
--- PRISMA Data Integrity Validation ---
[PASS] screening_log.csv: Column structure is correct.
[PASS] study_extraction.csv: Column structure is correct.
[PASS] paired_comparisons.csv: Column structure is correct.
[PASS] All paired IDs exist in the extraction ledger.
[PASS] Numeric metrics are synchronized (Baseline logic applied).
--- SUCCESS: All data artifacts verified for archive ---
```

### 3. Statistical Analysis (Saturation Hypothesis)

Compute paired statistical tests to evaluate performance saturation.

```bash
python scripts/analyze_saturation.py
```

**Validated results:**
- Number of pairs: 11
- p-value: 0.0244 (significant at Î± = 0.05)  
- Cliffâ€™s Delta: 0.5950 (large effect)

### 4. Operational Readiness Audit

Generate model-level readiness classifications.

```bash
python scripts/generate_orc.py
```

---

## ğŸ›  Operational Readiness Checklist (ORC)

Models are evaluated on a 5-point scale across the following criteria:

1. **Provenance** â€” Clear disclosure of dataset origin  
2. **Modality** â€” Diversity and independence of input units  
3. **Rigor** â€” Use of cross-validation or leave-one-out protocols  
4. **Bias Mitigation** â€” Avoidance of synthetic oversampling (e.g., SMOTE)  
5. **Transparency** â€” Explicit reporting of sample size (N)

A score of **â‰¥ 4 / 5** is required for a model to be classified as **CLINICAL READY**.

---

## ğŸ“„ Citation

If you use this repository or its findings, please cite:

```bibtex
@article{MukherjeePandey2025ADS,
  title   = {AI for Mental Health: A Survey of Models Leveraging Textual and Behavioral Data},
  author  = {Mukherjee, Priyangshu and Pandey, Khusboo},
  journal = {Systematic Review Repository},
  year    = {2025},
  url     = {https://github.com/auraflaa/Research-Paper_AI-for-Mental-Health-A-Survey-of-Models-Leveraging-Textual-and-Behavioral-Data}
}
```

---

**Maintainer:** Priyangshu Mukherjee
