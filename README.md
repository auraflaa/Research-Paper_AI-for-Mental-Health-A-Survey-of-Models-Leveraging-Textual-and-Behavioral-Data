# AI for Mental Health

## A Survey of Models Leveraging Textual and Behavioral Data

This repository contains the **complete reproducibility and governance package** for the systematic survey:

> **‚ÄúAI for Mental Health: A Survey of Models Leveraging Textual and Behavioral Data‚Äù**

The study examines AI-based detection of **Anxiety, Depression, and Stress (ADS)** using textual and behavioral signals, with a deliberate emphasis on **methodological rigor**, **evaluation validity**, and **operational readiness**, rather than headline accuracy gains alone.

This repository is designed to function as a **self-auditing research artifact**, suitable for long-term archival, independent verification, and downstream meta-research.

---

## üìå Abstract

Recent advances in deep learning for ADS detection frequently report substantial performance improvements over classical machine learning approaches. However, through a PRISMA-aligned systematic review and paired within-study statistical analysis, we identify a **performance saturation plateau**‚Äîwhere increasingly complex architectures yield **diminishing returns** once controls for **data leakage**, **evaluation protocol rigor**, and **sample provenance** are enforced.

Using paired contrasts across methodologically comparable studies, we observe statistically significant saturation:

* **Wilcoxon Signed-Rank Test:** p = 0.0244
* **Cliff‚Äôs Delta:** 0.5950 (large effect size)

To address the persistent gap between **research prototypes** and **deployable clinical systems**, we introduce an **Operational Readiness Checklist (ORC)**‚Äîa structured auditing framework for assessing whether a model satisfies minimum standards for responsible, real-world deployment.

---

## üìÇ Repository Structure

```text
Research-Paper_AI-for-Mental-Health/
‚îú‚îÄ‚îÄ data/                         # Frozen data layer (archive-safe)
‚îÇ   ‚îú‚îÄ‚îÄ screening_log.csv         # PRISMA screening records (N = 92)
‚îÇ   ‚îú‚îÄ‚îÄ study_extraction.csv      # Structured extraction & metrics (N = 27)
‚îÇ   ‚îú‚îÄ‚îÄ paired_comparisons.csv    # Paired contrasts for saturation testing
‚îÇ   ‚îî‚îÄ‚îÄ orc_report.csv            # Generated Operational Readiness audit
‚îÇ
‚îú‚îÄ‚îÄ scripts/                      # Automation & analysis engine
‚îÇ   ‚îú‚îÄ‚îÄ validate_prism.py         # Schema + ID + metric synchronization
‚îÇ   ‚îú‚îÄ‚îÄ analyze_saturation.py     # Wilcoxon test, Cliff‚Äôs Delta, plots
‚îÇ   ‚îî‚îÄ‚îÄ generate_orc.py           # Operational Readiness scoring
‚îÇ
‚îú‚îÄ‚îÄ figures/                      # Manuscript-ready assets
‚îÇ   ‚îú‚îÄ‚îÄ saturation_plot.pdf       # Performance vs. dataset size
‚îÇ   ‚îî‚îÄ‚îÄ prisma_flow.pdf           # PRISMA 2020 flow diagram
‚îÇ
‚îú‚îÄ‚îÄ documents/                    # Methodological governance artifacts
‚îÇ   ‚îú‚îÄ‚îÄ QA_Scoring_Rubric.pdf     # Study-level quality assessment rubric
‚îÇ   ‚îî‚îÄ‚îÄ ORC_Scoring_Guide.pdf     # Operational Readiness Checklist guide
‚îÇ
‚îú‚îÄ‚îÄ refs/                         # Primary study PDFs / metadata placeholders
‚îú‚îÄ‚îÄ requirements.txt              # Python dependencies
‚îú‚îÄ‚îÄ LICENSE                       # MIT License
‚îî‚îÄ‚îÄ README.md                     # Project overview & reproduction guide
```

---

## üöÄ Reproduction Workflow

### 1. Environment Setup

## Environment Setup (All Operating Systems)

### 1. Clone the Repository

```bash
git clone https://github.com/auraflaa/Research-Paper_AI-for-Mental-Health-A-Survey-of-Models-Leveraging-Textual-and-Behavioral-Data.git
cd Research-Paper_AI-for-Mental-Health-A-Survey-of-Models-Leveraging-Textual-and-Behavioral-Data
```

---

### 2. Create a Virtual Environment

#### Windows

```bash
python -m venv venv
```

#### macOS / Linux

```bash
python3 -m venv venv
```

---

### 3. Activate the Virtual Environment

#### Windows (PowerShell)

```bash
venv\Scripts\Activate.ps1
```

> If activation is blocked, run once:
>
> ```bash
> Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser
> ```

#### Windows (Command Prompt)

```bash
venv\Scripts\activate
```

#### macOS / Linux

```bash
source venv/bin/activate
```

You should now see `(venv)` in your terminal prompt.

---

### 4. Install Dependencies

```bash
pip install -r requirements.txt
```

---

### 5. Deactivate the Virtual Environment (Optional)

```bash
deactivate
```

### 6. PRISMA Data Integrity Validation

This step enforces schema locking, ID synchronization, and numeric consistency across all CSV artifacts.

```bash
python scripts/validate_prism.py
```

**Expected output:**

```
--- PRISMA Data Integrity Validation ---
[PASS] screening_log.csv: Column structure is correct.
[PASS] study_extraction.csv: Column structure is correct.
[PASS] paired_comparisons.csv: Column structure is correct.
[PASS] All paired IDs exist in the extraction ledger.
[PASS] Numeric metrics are synchronized (Baseline logic applied).
--- SUCCESS: All data artifacts verified for archive ---
```

---

### 7. Statistical Analysis ‚Äî Saturation Hypothesis

Paired non-parametric tests are used to evaluate whether performance gains saturate across increasing model complexity.

```bash
python scripts/analyze_saturation.py
```

**Validated results:**

* Number of paired contrasts: **11**
* **Wilcoxon Signed-Rank Test:** p = 0.0244
* **Cliff‚Äôs Delta:** 0.5950 (large effect)

The generated saturation plot is saved to:

```
figures/saturation_plot.pdf
```

---

### 8. Operational Readiness Audit (ORC)

This step generates a machine-readable readiness assessment for each extracted model.

```bash
python scripts/generate_orc.py
```

The output file:

```
data/orc_report.csv
```

classifies models into **CLINICAL READY** or **RESEARCH PROTOTYPE** categories based on transparent, rule-based criteria.

---

## üõ† Operational Readiness Checklist (ORC)

Each model is evaluated on a **5-point binary checklist**:

1. **Provenance** ‚Äî Dataset source clearly disclosed
2. **Modality Transparency** ‚Äî Input unit and granularity defined
3. **Evaluation Rigor** ‚Äî Cross-validation or subject-independent testing
4. **Bias Mitigation** ‚Äî No unsafe synthetic oversampling (e.g., SMOTE)
5. **Transparency** ‚Äî Explicit reporting of sample size (N)

* **Score ‚â• 4 / 5:** *CLINICAL READY*
* **Score < 4 / 5:** *RESEARCH PROTOTYPE*

> **Important:**
> ‚ÄúClinical Ready‚Äù denotes **methodological deployability**, not regulatory approval or clinical certification.

---

## üìÑ Citation

If you use this repository or its findings, please cite:

```bibtex
@misc{MukherjeePandey2025ADS,
  title        = {AI for Mental Health: A Survey of Models Leveraging Textual and Behavioral Data},
  author       = {Mukherjee, Priyangshu and Pandey, Khusboo},
  year         = {2025},
  note         = {Unpublished manuscript and reproducibility package},
  howpublished = {\\url{https://github.com/auraflaa/Research-Paper_AI-for-Mental-Health-A-Survey-of-Models-Leveraging-Textual-and-Behavioral-Data}}
}
```

---

## üë§ Maintainer

**Priyangshu Mukherjee**
B.Tech (Hons.), Computer Science & Engineering
RV University

* üìß Email: **[priyangshumukherjeebtech24@rvu.edu.in](mailto:priyangshumukherjeebtech24@rvu.edu.in)**
* üîó LinkedIn: [https://www.linkedin.com/in/priyangshu-mukherjee/](https://www.linkedin.com/in/priyangshu-mukherjee/)
